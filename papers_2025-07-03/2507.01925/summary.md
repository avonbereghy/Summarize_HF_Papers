# A Survey on Vision-Language-Action Models: An Action Tokenization
  Perspective

**Paper ID:** 2507.01925

**URL:** https://huggingface.co/papers/2507.01925

## Summary

## Executive Summary
The paper provides a comprehensive survey of **vision-language-action (VLA) models**, which have gained significant attention in recent years due to their potential to extend intelligence to the physical world. The authors observe that current VLA models can be unified under a single framework, where *vision and language inputs* are processed to produce a chain of **action tokens** that encode *actionable information*. The primary design choice distinguishing VLA models lies in how **action tokens** are formulated, which can be categorized into different types, including *language description*, *code*, and *affordance*. The survey aims to categorize and interpret existing VLA research through the lens of **action tokenization**, highlighting *strengths and limitations* of each token type and identifying areas for improvement.

## Key Contributions and Findings
* **Unification of VLA Models**: The authors propose a unified framework for VLA models, where *vision and language inputs* are processed to produce a chain of **action tokens** that encode *actionable information*.
* **Action Tokenization**: The survey categorizes existing VLA research into different **action token** types, including *language description*, *code*, *affordance*, and *latent representation*, and analyzes their *strengths and limitations*.
* **Identification of Areas for Improvement**: The authors identify areas for improvement in current VLA models, including the need for more *effective action tokenization* and *better integration of vision and language inputs*.
* **Synthesized Outlook**: The survey provides a synthesized outlook on the broader evolution of VLA models, highlighting *underexplored yet promising directions* and contributing guidance for future research.
* **Guidance for Future Research**: The authors offer guidance for future research, hoping to bring the field closer to **general-purpose intelligence**.

## Methodology Overview
The methodology involves a systematic review and analysis of existing VLA research, using **natural language processing** and *machine learning* techniques to categorize and interpret different **action token** types. The authors employ a *taxonomy-based approach* to classify VLA models into different categories, and use **data visualization** techniques to illustrate the relationships between different **action token** types.

## Results and Performance
The survey provides a comprehensive overview of the current state of VLA research, highlighting **key performance metrics** such as *accuracy* and *efficiency*. The authors compare the performance of different **action token** types, including *language description* and *code*, and analyze their *strengths and limitations*. The results show that **VLA models** have achieved significant progress in recent years, but still face challenges in terms of *generalizability* and *robustness*.

## Limitations and Future Work
The survey mentions several limitations of current VLA models, including the need for more *effective action tokenization* and *better integration of vision and language inputs*. Potential future directions include the development of more *advanced action tokenization techniques*, and the exploration of new **application domains** such as *robotics* and *autonomous driving*. The authors also highlight the need for more *diverse and representative datasets* to support the development of VLA models.

## Practical Applications
The survey highlights several potential real-world applications of VLA models, including *robotics*, *autonomous driving*, and *human-computer interaction*. VLA models have the potential to enable **more efficient and effective interaction** between humans and machines, and to support the development of more **intelligent and autonomous systems**. The authors also mention the potential for VLA models to be used in *education* and *training*, where they can help to *improve learning outcomes* and *enhance student engagement*.

---

**Authors:** Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang
