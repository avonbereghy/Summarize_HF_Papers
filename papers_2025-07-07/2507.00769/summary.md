# LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative
  Writing

**Paper ID:** 2507.00769

**URL:** https://huggingface.co/papers/2507.00769

## Summary

## Executive Summary
The paper introduces **LitBench**, a benchmark and dataset for evaluating creative writing generated by *large language models* (**LLMs**). The authors aim to address the challenge of evaluating open-ended narratives by providing a standardized benchmark and paired dataset, comprising a held-out test set and a training corpus of human preference labels. This enables the development of reliable automated evaluation methods, including *zero-shot LLM judges* and *trained reward models*. The paper highlights the importance of **robust evaluation** for creative writing and presents **LitBench** as a vetted resource for optimizing creative writing systems.

## Key Contributions and Findings
* **Benchmark Development**: The authors create a standardized benchmark, **LitBench**, which includes a held-out test set of 2,480 *debiased* story comparisons and a 43,827-pair training corpus of human preference labels.
* **Evaluation of LLM Judges**: The paper evaluates *zero-shot LLM judges*, identifying **Claude-3.7-Sonnet** as the strongest off-the-shelf judge, with a 73% agreement with human preferences.
* **Training Reward Models**: The authors train **Bradley Terry** and *generative reward models*, which attain an accuracy of 78%, outperforming all off-the-shelf judges.
* **Human Study Validation**: An online human study confirms that the trained reward models consistently align with human preferences in novel *LLM-generated stories*.
* **Resource Release**: The authors release **LitBench** and the trained reward models, providing a vetted resource for reliable automated evaluation and optimization of creative writing systems.

## Methodology Overview
The methodology involves **data collection** from Reddit, followed by *data preprocessing* to create a debiased dataset. The authors then develop a **benchmark** by creating a held-out test set and a training corpus of human preference labels. They employ *machine learning techniques*, including **Bradley Terry** and *generative models*, to train reward models and evaluate their performance.

## Results and Performance
The key results show that the trained reward models achieve a **78% accuracy**, outperforming all off-the-shelf judges, including the strongest judge, **Claude-3.7-Sonnet**, which reaches a **73% agreement** with human preferences. The *online human study* further confirms the validity of the trained reward models, with a *significant correlation* between model rankings and human preferences.

## Limitations and Future Work
The paper does not explicitly mention limitations, but potential future directions include:
* Expanding the dataset to include more diverse sources and genres
* Developing more advanced *machine learning models* to improve evaluation accuracy
* Investigating the application of **LitBench** to other creative writing tasks, such as *poetry generation*

## Practical Applications
The development of **LitBench** and the trained reward models has significant implications for *creative writing systems*, enabling reliable automated evaluation and optimization. Potential real-world applications include:
* **Content generation**: Improving the quality and coherence of automatically generated content, such as *articles* and *stories*.
* **Language learning**: Developing more effective language learning tools that incorporate creative writing exercises and feedback mechanisms.
* **Artistic collaboration**: Enabling humans and *LLMs* to collaborate on creative writing projects, with the models providing suggestions and feedback.

---

**Authors:** Daniel Fein, Sebastian Russo, Violet Xiang, Kabir Jolly, Rafael Rafailov, Nick Haber
