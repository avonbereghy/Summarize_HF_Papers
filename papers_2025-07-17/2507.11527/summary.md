# DrafterBench: Benchmarking Large Language Models for Tasks Automation in
  Civil Engineering

**Paper ID:** 2507.11527

**URL:** https://huggingface.co/papers/2507.11527

## Summary

## Executive Summary
The proposed **benchmarking framework**, DrafterBench, is designed to evaluate the performance of *Large Language Models (LLMs)* in automating tasks in civil engineering, specifically in technical drawing revision. This **comprehensive evaluation** framework assesses the ability of LLMs to interpret *intricate instructions*, leverage *prior knowledge*, and adapt to *dynamic instruction quality*. By providing a detailed analysis of **task accuracy** and *error statistics*, DrafterBench aims to offer deeper insights into the capabilities of LLMs and identify areas for improvement in integrating these models into *engineering applications*.

## Key Contributions and Findings
* **Benchmark Design**: DrafterBench is a carefully designed benchmark that includes *twelve types of tasks* summarized from real-world drawing files, with *46 customized functions/tools* and *1920 tasks* in total.
* **Comprehensive Evaluation**: The framework comprehensively assesses distinct capabilities in *structured data comprehension*, **function execution**, *instruction following*, and **critical reasoning**.
* **Open-Source Availability**: DrafterBench is an *open-source benchmark*, making it accessible to researchers and developers, and is available at https://github.com/Eason-Li-AIS/DrafterBench.
* **Detailed Analysis**: The toolkit provides a detailed analysis of **task accuracy** and *error statistics*, aiming to provide deeper insights into the capabilities of LLMs and identify areas for improvement.
* **Real-World Applicability**: DrafterBench has the potential to be applied to real-world *civil engineering tasks*, making it a valuable resource for *industry professionals* and *researchers*.

## Methodology Overview
The methodology of DrafterBench involves **task design**, where *real-world drawing files* are used to create *twelve types of tasks*. The framework also includes **function implementation**, where *46 customized functions/tools* are developed to support the tasks. Additionally, **evaluation metrics** are used to assess the performance of LLMs, including *task accuracy* and *error statistics*. The framework utilizes *implicit policy awareness* to adapt to *dynamic instruction quality*.

## Results and Performance
The results of DrafterBench show the **performance** of LLMs in automating tasks in civil engineering, with **metrics** such as *task accuracy* and *error statistics* providing insights into the capabilities of these models. The framework allows for *comparisons* between different LLMs, enabling researchers to identify the most effective models for specific tasks. The **benchmark** also provides a detailed analysis of the **strengths** and *weaknesses* of LLMs in *technical drawing revision*.

## Limitations and Future Work
The limitations of DrafterBench include the need for further *evaluation* and *validation* of the framework, as well as the potential for *bias* in the *task design*. Future work may involve *expanding* the framework to include additional tasks and functions, as well as *improving* the *evaluation metrics* to provide more accurate assessments of LLM performance.

## Practical Applications
The practical applications of DrafterBench include *automating tasks* in civil engineering, such as *technical drawing revision*, and *improving* the *efficiency* and *accuracy* of these tasks. The framework also has the potential to be applied to other *industry domains*, such as *architecture* and *construction*. Additionally, DrafterBench may be used to *train* and *evaluate* LLMs for use in *real-world applications*, making it a valuable resource for *industry professionals* and *researchers*.

---

**Authors:** Yinsheng Li, Zhen Dong, Yi Shao
