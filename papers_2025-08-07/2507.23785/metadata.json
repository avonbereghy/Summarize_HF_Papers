{
  "id": "2507.23785",
  "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
  "authors": [
    {
      "_id": "6892cf048da45ffb0a2b24b6",
      "user": {
        "_id": "6237f25f16004228e6c74e01",
        "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
        "isPro": false,
        "fullname": "Bowen Zhang",
        "user": "BwZhang",
        "type": "user"
      },
      "name": "Bowen Zhang",
      "status": "claimed_verified",
      "statusLastChangedAt": "2025-08-06T19:16:22.835Z",
      "hidden": false
    },
    {
      "_id": "6892cf048da45ffb0a2b24b7",
      "name": "Sicheng Xu",
      "hidden": false
    },
    {
      "_id": "6892cf048da45ffb0a2b24b8",
      "name": "Chuxin Wang",
      "hidden": false
    },
    {
      "_id": "6892cf048da45ffb0a2b24b9",
      "name": "Jiaolong Yang",
      "hidden": false
    },
    {
      "_id": "6892cf048da45ffb0a2b24ba",
      "name": "Feng Zhao",
      "hidden": false
    },
    {
      "_id": "6892cf048da45ffb0a2b24bb",
      "name": "Dong Chen",
      "hidden": false
    },
    {
      "_id": "6892cf048da45ffb0a2b24bc",
      "name": "Baining Guo",
      "hidden": false
    }
  ],
  "abstract": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
  "url": "https://huggingface.co/papers/2507.23785",
  "pdf_url": "https://arxiv.org/pdf/2507.23785.pdf"
}