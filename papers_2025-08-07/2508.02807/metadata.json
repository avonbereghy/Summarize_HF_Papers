{
  "id": "2508.02807",
  "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
  "authors": [
    {
      "_id": "68944b9e741a16f544fbcfec",
      "name": "Tongchun Zuo",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcfed",
      "name": "Zaiyu Huang",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcfee",
      "name": "Shuliang Ning",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcfef",
      "name": "Ente Lin",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcff0",
      "name": "Chao Liang",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcff1",
      "name": "Zerong Zheng",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcff2",
      "name": "Jianwen Jiang",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcff3",
      "name": "Yuan Zhang",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcff4",
      "name": "Mingyuan Gao",
      "hidden": false
    },
    {
      "_id": "68944b9e741a16f544fbcff5",
      "name": "Xin Dong",
      "hidden": false
    }
  ],
  "abstract": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. In the second stage, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
  "url": "https://huggingface.co/papers/2508.02807",
  "pdf_url": "https://arxiv.org/pdf/2508.02807.pdf"
}