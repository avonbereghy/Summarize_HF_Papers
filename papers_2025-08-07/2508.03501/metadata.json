{
  "id": "2508.03501",
  "title": "Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning",
  "authors": [
    {
      "_id": "68946454741a16f544fbd048",
      "name": "Alexander Golubev",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd049",
      "name": "Maria Trofimova",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd04a",
      "name": "Sergei Polezhaev",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd04b",
      "name": "Ibragim Badertdinov",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd04c",
      "name": "Maksim Nekrashevich",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd04d",
      "name": "Anton Shevtsov",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd04e",
      "name": "Simon Karasik",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd04f",
      "name": "Sergey Abramov",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd050",
      "name": "Andrei Andriushchenko",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd051",
      "name": "Filipp Fisin",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd052",
      "name": "Sergei Skvortsov",
      "hidden": false
    },
    {
      "_id": "68946454741a16f544fbd053",
      "name": "Boris Yangel",
      "hidden": false
    }
  ],
  "abstract": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.",
  "url": "https://huggingface.co/papers/2508.03501",
  "pdf_url": "https://arxiv.org/pdf/2508.03501.pdf"
}