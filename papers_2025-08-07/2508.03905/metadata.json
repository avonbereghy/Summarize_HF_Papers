{
  "id": "2508.03905",
  "title": "Sotopia-RL: Reward Design for Social Intelligence",
  "authors": [
    {
      "_id": "6894156a741a16f544fbced2",
      "name": "Haofei Yu",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced3",
      "name": "Zhengyang Qi",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced4",
      "name": "Yining Zhao",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced5",
      "name": "Kolby Nottingham",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced6",
      "name": "Keyang Xuan",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced7",
      "name": "Bodhisattwa Prasad Majumder",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced8",
      "name": "Hao Zhu",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbced9",
      "name": "Paul Pu Liang",
      "hidden": false
    },
    {
      "_id": "6894156a741a16f544fbceda",
      "name": "Jiaxuan You",
      "hidden": false
    }
  ],
  "abstract": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
  "url": "https://huggingface.co/papers/2508.03905",
  "pdf_url": "https://arxiv.org/pdf/2508.03905.pdf"
}