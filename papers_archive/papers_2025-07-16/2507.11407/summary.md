# EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and
  Reasoning Modes

**Paper ID:** 2507.11407

**URL:** https://huggingface.co/papers/2507.11407

## Summary

## Executive Summary
The EXAONE 4.0 model series introduces a **unified large language model** that integrates *non-reasoning* and *reasoning modes*, achieving both excellent usability and advanced reasoning abilities. This technical report highlights the model's **multilingual capabilities**, including support for Spanish, English, and Korean, and its potential to pave the way for the **agentic AI era**. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model and a small-size 1.2B model, both of which demonstrate *superior performance* compared to open-weight models in their class.

## Key Contributions and Findings
* **Model Architecture**: The EXAONE 4.0 model series integrates a *non-reasoning mode* and a *reasoning mode* to achieve both excellent usability and advanced reasoning abilities, making it a **unified large language model**.
* **Multilingual Capabilities**: The model extends its *multilingual capabilities* to support Spanish, in addition to English and Korean, making it a more *versatile* and *accessible* tool.
* **Agentic Tool Use**: The EXAONE 4.0 model incorporates essential features such as *agentic tool use*, which enables the model to interact with its environment in a more *intelligent* and *autonomous* way.
* **Model Sizes**: The model series consists of two sizes: a mid-size 32B model optimized for *high performance*, and a small-size 1.2B model designed for *on-device applications*.
* **Public Availability**: The EXAONE 4.0 models are publicly available for *research purposes* and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.

## Methodology Overview
The methodology used to develop the EXAONE 4.0 model series involves **large-scale language modeling** using *transformer architectures* and **pre-training** on large datasets. The model is trained using a combination of *masked language modeling* and *next sentence prediction* tasks, and is fine-tuned using *domain-specific datasets*.

## Results and Performance
The EXAONE 4.0 model series demonstrates **superior performance** compared to open-weight models in its class, with *state-of-the-art* results on several *benchmark datasets*. The model remains **competitive** even against *frontier-class models*, with *impressive* results on tasks such as *language translation* and *question answering*. The **metrics** used to evaluate the model's performance include *perplexity*, *accuracy*, and *F1 score*.

## Limitations and Future Work
The limitations of the EXAONE 4.0 model series include its *computational requirements*, which can be *prohibitive* for some applications. Potential future directions include *improving the model's efficiency*, *expanding its multilingual capabilities*, and *exploring its applications* in areas such as *natural language processing* and *human-computer interaction*.

## Practical Applications
The EXAONE 4.0 model series has several potential **practical applications**, including *language translation*, *question answering*, and *text summarization*. The model's *agentic tool use* capabilities also make it suitable for applications such as *virtual assistants* and *chatbots*. Additionally, the model's *multilingual capabilities* make it a valuable tool for *cross-lingual communication* and *cultural exchange*.

---

**Authors:** LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun
